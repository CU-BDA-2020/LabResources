{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab05 Exercises\n",
    "\n",
    "These exercises are in lieu of an assignment (because of the February break). Some of these exercises will be part of future assignments. Solutions won't be provided, but we'll be happy to help you work on these problems in office hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The normal-normal conjugate model\n",
    "\n",
    "In Lecture 08 we covered basic inference with the normal (Gaussian) distribution.  Without derivation, I claimed that a normal prior is the conjugate prior for a likelihood function based on data with a normal distribution for the additive noise or error terms.  Verify this claim *numerically*.  The following instructions use the notation in the Lecture 08 slides, for the problem of inferring $\\mu$ with $\\sigma$ considered *known*.\n",
    "\n",
    "For this exercise, write a separate script named \"NormNorm.py\" implementing your solution (this is to give you some exposure to Python scripts vs. notebooks; you may want to experiment with the [Spyder IDE](https://www.spyder-ide.org/) as you work on this—it's included with Anaconda Python and you can launch it from the command line via `spyder`).  At the end of your notebook, run the script in the notebook (using IPython's `run` command; make sure you have `ion()` in the notebook or the script).  \n",
    "\n",
    "This exercise asks you to write some unit tests (in the \"NormNorm.py\" file). To execute the tests, run the `pytest` command-line program in your notebook using \"`!pytest NormNorm.py`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1:\n",
    "\n",
    "> Calculate and plot the posterior PDF using the analytic formula from lecture:\n",
    "> * Use the `scipy.stats.norm` distribution object to generate a single sample of $N$ observations, $d_i$, following the model in the lecture.  Pick your own $N$, and your own \"true\" values of the parameters $\\mu$ and $\\sigma$ for the observations.\n",
    "> * Pick a prior mean, $\\mu_0$, and standard deviation, $w_0$, defining a normal prior.  Plot the posterior PDF for $\\mu$ using the formula presented in class for the conjugate posterior (the formula with the quantity $B$ specifying how much the posterior shrinks toward the prior).  Use the numpy `linspace` function to make an array of $\\mu$ values over which you'll evaluate the PDF.  You may use either the scipy.stats `norm` object, or explicit calculation (with `exp`, etc.), to evaluate the PDF.  Use a thick solid curve for the plot (say, with lw=2 or 3 in the matplotlib `plot` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2:\n",
    "\n",
    "> Now explicitly calculate and plot the posterior PDF from the prior and likelihood: \n",
    "* Use the same grid of $\\mu$ values used for Problem 4.1.\n",
    "* Evaluate the normal prior and the likelihood function on the grid.\n",
    "* Calculate the prior $\\times$ likelihood, and normalize it using the trapezoid rule. Code the trapezoid rule (covered in Lec11) explicitly; don't use `numpy.trapz` or `scipy.integrate.trapz`.\n",
    "* Plot the resulting normalized PDF on the same axes as Problem 4.1.  Use a dashed line style (and optionally transparency, via the `alpha` argument to `plot`) so that both curves are visible (they should overlap!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3:\n",
    "\n",
    "> Create test cases that verify elements of your computation:\n",
    "* Create a case that checks whether your trapezoid rule integration matches the result given by `numpy.trapz`.\n",
    "* Create a case that checks whether the two posterior PDFs match over the grid of $\\mu$ values.\n",
    "* Include a `pytest` run in your notebook that verifies the tests pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change of variables from probability to odds\n",
    "\n",
    "Consider inference problems with Bernoulli outcomes with success probability $\\alpha$ (e.g., estimating $\\alpha$ from binary sequence data using a Bernoulli process model, or from count data using a binomial distribution model). Suppose the prior for $\\alpha$ is some specified function $f(\\alpha)$. Define the single-trial odds in favor of success as\n",
    "\\begin{align}\n",
    "o &\\equiv \\frac{\\alpha}{1-\\alpha}\\\\\n",
    "  &\\equiv O(\\alpha).\n",
    "\\end{align}\n",
    "\n",
    "> * What is the inverse mapping, $A(o)$, that gives the value of $\\alpha$ corresponding to a given value of $o$?\n",
    "> * What PDF for $o$, $g(o)$, corresponds to a given PDF for $\\alpha$, $f(\\alpha)$? Be sure that $g(o)$ is expressed in terms of $o$, i.e., $\\alpha$ should not appear. Compute any derivatives that appear in the formula.\n",
    "> * A uniform prior PDF for $\\alpha$ corresponds to $f(\\alpha) = 1$ (this is normalized over $[0,1]$). What prior for $o$ corresponds to this uniform prior for $\\alpha$?\n",
    "> * Verify that the PDF for $o$ you just derived is normalized.\n",
    "> * The uniform prior for $\\alpha$ assigns probability $1/2$ for $\\alpha$ being in the interval $[0,1/2]$. Identify the corresponding interval for $o$, and show that the PDF for $o$ you just computed assigns probability $1/2$ to that interval.\n",
    "\n",
    "For this exercise you'll need to use the PDF transformation rule described in Lec09 (\"change of variables\"). For online presentations on this topic, see:\n",
    "* [Lesson 22: Functions of One Random Variable – PSU STAT 414 / 415](https://onlinecourses.science.psu.edu/stat414/node/128)\n",
    "* [Probability/Transformation of Probability Densities – Wikibooks](https://en.wikibooks.org/wiki/Probability/Transformation_of_Probability_Densities#Function_of_a_Random_Variable_(n=1,_m=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Cauchy distribution\n",
    "\n",
    "*We currently plan on including this problem in Assignment04.*\n",
    "\n",
    "The *Cauchy distribution* is a special case of the Student's $t$ distribution covered in Lec08; it corresponds to Student's $t$ with degrees of freedom parameter $\\nu=1$. It has an undefined mean and an infinite variance.  It is troublesome to work with in frequentist statistics.  Even it's maximum likelihood estimator has complicated sampling properties that pose not just computational challenges, but conceptual ones (the best frequentist methods require adopting something called the *conditional frequentist approach*, related to the likelihood principle).  In Bayesian inference, it poses no conceptual difficulties, but it must be handled numerically (which you'll do in a future assignment).\n",
    "\n",
    "The Cauchy distribution is known in physics as the *Lorentzian distribution*, where in certain circumstances it describes the profile of spectral lines, and the distribution of particle mass peaks in accelerator experiments.  It also appears in problems where the ratio of two quantities with normal errors is of interest; when the quantities are uncorrelated with zero mean, the PDF for the ratio is a Cauchy distribution.  As noted above, the Student's $t$ distribution with 1 degree of freedom is a Cauchy distribution.  It also arises in geometric inference problems, as you will see in this exercise.\n",
    "\n",
    "You can find basic information about the Cauchy distribution [on Wikipedia](http://en.wikipedia.org/wiki/Cauchy_distribution) and in the [NIST Engineering Statistics Handbook](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm).\n",
    "\n",
    "**Exercise:** A rod of radioactive material is a distance $d$ behind a barrier (e.g., this could be a fuel rod inside a nuclear reactor). A narrow sensor strip is on the barrier, oriented orthogonal to the rod (and not near either of its ends). The rod is at an unknown position, $x_0$, along the sensor.  The sensor records the locations of $N$ gamma rays emitted by the rod, denoted $x_i$ (for $i=1$ to $N$).  We'll assume that $d$ is small compared to the length of the sensor, so that the sensor may be considered essentially infinite in length.\n",
    "\n",
    "The geometry is shown in the following figure, oriented looking down along the rod (shown as a large dot).\n",
    "\n",
    "<img src=\"CauchyGeometry.png\"/>\n",
    "\n",
    "> Assume the rod emits gamma rays isotropically (i.e., with a uniform distribution in the angle $\\theta$; consider only angles corresponding to detectable gamma rays).  Show that the PDF for the detected location of a single gamma ray, $x$, is a Cauchy distribution with location parameter $x_0$ and scale parameter $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predicting a Bernoulli outcome\n",
    "\n",
    "Suppose you have a parametric model for some available data, $D$; denote the parameters by $\\theta$ (as usual!). The posterior PDF for $\\theta$ is\n",
    "$$\n",
    "p(\\theta|D,\\mathcal{C}) = \\frac{p(\\theta|\\mathcal{C})\\, p(D|\\theta,\\mathcal{C})}{p(D|\\mathcal{C})}.\n",
    "$$\n",
    "Now suppose you'd like to predict future data, $D'$. From a Bayesian perspective, you would do this by computing the **posterior predictive distribution**, $p(D'|D,\\mathcal{C})$.\n",
    "\n",
    "It's probably not immediately clear how to compute that. On the other hand, since we are working in the context of a parametric model, if we knew the value of the parameters, $\\theta$, then we'd be able to compute a predictive distribution: it would just be the (parametric) sampling distribution for the *new* data, $p(D'|\\theta,\\mathcal{C})$. This suggests using the LTP to compute the posterior predictive distribution, as follows:\n",
    "\\begin{align}\n",
    "p(D'|D)\n",
    "  &= \\int d\\theta\\, p(D',\\theta|D) \\qquad ||\\,\\mathcal{C}\\\\\n",
    "  &= \\int d\\theta\\, p(\\theta|D)\\, p(D'|\\theta,D)\\\\\n",
    "  &= \\int d\\theta\\, p(\\theta|D)\\, p(D'|\\theta).\n",
    "\\end{align}\n",
    "For the last line, we took advantage of the fact that knowing $\\theta$ determines the sampling distribution for any data, so conditioning on $D$ doesn't affect $p(D'|\\theta,D)$.\n",
    "\n",
    "On that last line, the first factor is the posterior PDF for $\\theta$ based on the available data, $D$.  The second factor is the predictive probability for $D'$ if we knew $\\theta$ (i.e., the sampling distribution for $D'$). So this equation tells us the predictive distribution is an average of the parametric sampling distributions—averaged using the posterior for $\\theta$ as the averaging weight.\n",
    "\n",
    "Apply this to a simple example: predicting that the result of one additional trial will be heads, using the binomial distribution as a model for counts of $n$ heads in $N$ trials.\n",
    "\n",
    "> * Denote the probability for heads in a single trial by $\\alpha$ (as usual!). If you've seen $n$ heads in $N$ trials, what is the posterior for $\\alpha$? (Just write this down from your notes; you needn't derive it anew.) In the context of the posterior predictive distribution above, this PDF is $p(\\theta|D)$ (with $\\theta\\rightarrow\\alpha$, and $D\\rightarrow n$).\n",
    "> * If you knew $\\alpha$, what would be the probability for seeing $n'=1$ head in $N=1$ new trial? In the context of the posterior predictive distribution above, this (simple!) PMF is $p(D'|\\theta)$.\n",
    "> * Compute the posterior predictive probability for heads, $p(n'=1|n)$.\n",
    "\n",
    "*Hints:* Keep the beta function formula handy. You are reproducing a very famous calculation done by Laplace; the result is known as *Laplace's rule of succession*. Think about what it says as $n$ and $N$ get large, and for the case where $N=0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
